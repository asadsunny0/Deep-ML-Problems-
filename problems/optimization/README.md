# Optimization Problems

Problems related to optimization algorithms used in training machine learning models.

## Topics Covered

- Gradient descent variants (SGD, Mini-batch GD)
- Advanced optimizers (Adam, RMSprop, AdaGrad)
- Momentum
- Learning rate scheduling
- Convergence analysis
- Convex optimization

## Coming Soon

- Stochastic Gradient Descent (SGD)
- Mini-batch Gradient Descent
- Momentum Implementation
- Adam Optimizer
- RMSprop Optimizer
- AdaGrad Optimizer
- Learning Rate Decay
- Convex Function Optimization

## Prerequisites

- Understanding of calculus (gradients)
- Basic optimization concepts
- Familiarity with neural network training
- Python or JavaScript programming

## Learning Resources

- "Convex Optimization" by Stephen Boyd
- "An overview of gradient descent optimization algorithms" by Sebastian Ruder
- Deep Learning Book, Chapter 8: Optimization for Training Deep Models
- CS229: Machine Learning course materials
